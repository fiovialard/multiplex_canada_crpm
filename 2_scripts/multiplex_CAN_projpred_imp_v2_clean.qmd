---
title: "Multiplex Canada Predictive Projection"
format: 
  html:
    embed-resources: true
editor: visual
Author: "Fio Vialard"
---

In this analysis we select candidate submodels from the reference model containing all variables ($p=$ 20) using predictive projection on the training (development) data. Then, we validate their performance from predicted (i.e sensitivity/specificity) or fitted (i.e AUC) values of the posterior of each submodel applied to testing (validation) data.

Note: the final performance measures may change slightly from the manuscript measures because the sampling frame changes for each run and because I used 5-fold CV to reduce rendering time.

## Set-up

```{r}
knitr::opts_knit$set(root.dir = "~/R files/multiplex canada") 
# to ensure that the project is at the root directory
knitr::opts_chunk$set(echo = TRUE, warning = F, fig.width=14, fig.height=18, message = F)
```

### Packages

```{r}
library(projpred) # main package
library(brms) # for bayesian statistics
library(tidyverse) # for tidy format
library(bayesplot) # to plot bayesian models
theme_set(bayesplot::theme_default(base_family = "sans"))
require(mice)
# require(loo)
```

### Load imputed data and functions

```{r}
mice1l<- read.csv("3_intermediate/imputed/imputed_data_long_no_prom_1l.csv")
mice_obj<- as.mids(mice1l)  # load the imputed data
source("2_scripts/00_functions.R") # functions
```

## Imputed data

Imputated data using 1-level predictive selection for binary variables.

### Select training vs testing dataset

```{r}
pooled.imp<- complete(mice_obj) # pool imputed data


unique_ids_imp<-pooled.imp$p_id

# Stratified sampling using dplyr
training_set_imp <- pooled.imp %>%
  group_by(site) %>%
  sample_frac(0.75, replace = FALSE) %>% 
  ungroup() %>% 
  arrange(p_id)

sampled_ids_imp <- training_set_imp$p_id

# testing set from the non-selected variables 
testing_set_imp <- pooled.imp %>% 
  filter(!p_id %in% sampled_ids_imp)


## check
length(unique_ids_imp) == length(unique(append(training_set_imp$p_id, testing_set_imp$p_id)))
```

### Fitting a reference model

First we create a column with outcome of interest (STBBI status)

```{r}

training_set_imp <- training_set_imp %>% 
  mutate(stbbi.pos = case_when(
    hcv.pos %in% 1~1,
    hiv.pos %in% 1~1,
    syphilis.pos %in% 1 ~1,
    T ~0),
    TypePastSTD = case_when(
      TypePastSTD %in% "None" ~ as.factor(TypePastSTD),
      TypePastSTD %in% "6-Other" ~ "Not specified",
      T ~ "Specified"
    )) %>%
  select(- c(new.hcv, new.syphilis, new.hiv, p_id, Date.Recruited, syphilis.pos, hcv.pos, hiv.pos)) %>%
   mutate_if(is.character, as.factor)


summary(training_set_imp)

```

**Note:** There are `r nrow(training_set_imp$stbbi.pos==1)` stbbi positive individuals in the training sample

Same for testing

```{r}
testing_set_imp <- testing_set_imp %>%
mutate(stbbi.pos = case_when(
    hcv.pos %in% 1~1,
    hiv.pos %in% 1~1,
    syphilis.pos %in% 1 ~1,
    T ~0),
    TypePastSTD = case_when(
      TypePastSTD %in% "None" ~ as.factor(TypePastSTD),
      TypePastSTD %in% "6-Other" ~ "Not specified",
      T ~ "Specified"
    )) %>%
  select(- c(new.hcv, new.syphilis, new.hiv, p_id, Date.Recruited, syphilis.pos, hcv.pos, hiv.pos)) %>%
   mutate_if(is.character, as.factor)

## check
ncol(testing_set_imp) == ncol(training_set_imp)

summary(testing_set_imp)
```

There are `r nrow(testing_set_imp$stbbi.pos==1)` infected individuals in the testing dataset.

Assign priors and formula

```{r}

xvar <- training_set_imp %>% 
  select(-c(stbbi.pos)) %>% 
  colnames() 
# Prior guess for the number of relevant (i.e., non-zero) regression
# coefficients:
p0 <- 5
# Number of observations:
N <- nrow(training_set_imp)
# number of regression coefficients 
D <- length(xvar)
# Hyperprior scale for tau, the global shrinkage parameter (note that for the
# Gaussian family, 'rstanarm' will automatically scale this by the residual
# standard deviation):
tau0 <- p0 / (D - p0) * 1 / sqrt(N)
prior_hs <- horseshoe(scale_global = tau0)

formula <- reformulate(setdiff(c(xvar), "stbbi.pos"), response = "stbbi.pos")

model_imp_hs <- brms::brm(formula,
                       data = training_set_imp,
                       family = bernoulli(link = "logit"), # binomial data
                       prior = set_prior(prior_hs),
                       seed = 0901,
                       verbose = F, 
                       refresh=0)

```

#### LOO measures

```{r}
#we load loo here because it interacts with brms and projpred in a way that change the results. We then need to unload it when we are done
require(loo)

model0_imp <- update(model_imp_hs, formula = stbbi.pos ~ 1,  verbose = F, seed=0901)
loo0 <- loo(model0_imp, save_psis = T)

loo_imp_hs <- loo(model_imp_hs, save_psis = TRUE)

loo_imp_hs

loo_compare(loo0, loo_imp_hs)

detach("package:loo", unload=TRUE)
# check that it was removed
 (.packages())
```

All estimates are ok. Not much difference in elpd with the normal prior model. There is a difference of more than 4 between model0 and reference model which can be interpreted as: the predictors provide information in the model.

### Posterior predictive check

```{r}
ref_obj_hs<- model_imp_hs

prj_ref <- project(  
  ref_obj_hs,    
  verbose = F,
  predictor_terms = xvar
  )

prj_predict <- proj_predict(prj_ref)

# Using the 'bayesplot' package: 
ppc_imp <- ppc_dens_overlay(y = training_set_imp$stbbi.pos, yrep = prj_predict)

ppc_imp

# turn into matrix 
prj_mat <- as.matrix(prj_ref)

# extract and clean the variable names

col <- colnames(prj_mat[,2:30])
#clean colnames
new_col<- str_replace_all(col, "(b_|\\d+-)| CAD", "") %>%
  str_replace_all("(?<!\\b(?:HCV|HIV|STD|CAD))(?<=\\w)(?=[A-Z])|(?<=\\D)(?=\\d)", " ") %>% 
  str_replace_all(" 2; 000", "2000") %>% 
  str_replace_all("H I V", "HIV") %>% 
  str_replace_all("H C V", "HCV") %>% 
  str_replace_all("S T D", "STD")

# to see the posterior distribution of each beta coefficient 
mcmc_imp <- mcmc_areas(prj_mat[,2:30])+
  scale_y_discrete(label= new_col)+ # add new column names
  xaxis_text(size= 20)+
  yaxis_text(size = 20)

mcmc_imp
```

### Variable selection

#### Fast horseshoe prior

```{r, fig.cap = "Submodels accuracy using rapid variable selection. The Y-axis represents estimated log predictive density (elpd) which is a performance statistic. The dashed red line represents the reference model performance"}
model_fast_imp_hs <- cv_varsel(
  model_imp_hs, 
  validate_search = F,
  method = "L1",
  verbose = FALSE, # remove the text
)

model_fast_imp_hs

plot(model_fast_imp_hs, stats = "elpd", ranking_nterms_max = NA)
```

Reaches the model performance after 6 predictors.

#### 5-fold cross validation

For the manuscript, I used the k-fold=n validation but to reduce rendering time, I use 5-fold validation in this version

```{r, fig.cap="Submodels accuracy using rapid variable selection. Y axis values represent the DIFFERENCE in performance with the reference model. The percentage below each submodel represents the cross-validation ranking of predictors"}
cv_fits_imp <- run_cvfun(
  model_imp_hs  # 5k is default
)

# Final cv_varsel() run:
model_imp_nk <- cv_varsel(
  model_imp_hs,
  cv_method = "kfold",
  cv_fits = cv_fits_imp,
  method = "forward",
  nterms_max = 15,
  verbose = F
)

model_imp_nk

performance_imp <- plot(model_imp_nk, stats = "elpd", delta = T)
performance_imp
```

#### Variable selection k-fold = sample size

Used this method because LOO package is not supported within projpred

```{r, fig.cap="Submodels accuracy using rapid variable selection. Y axis values represent the DIFFERENCE in performance with the reference model. The percentage below each submodel represents the cross-validation ranking of predictors"}
# cv_fits <- run_cvfun(
#   model_imp_hs,
#   K = nrow(training_set_imp))
# 
# model_imp_nk <- cv_varsel(
#   model_imp_hs,
#   cv_method = "kfold",
#   cv_fits = cv_fits,
#   nterms_max = 15, # to be safe
#   method = "forward",
#   verbose = F
# )
# 
# model_imp_nk
# 
# performance_imp <- plot(model_imp_nk, stats = "elpd", delta = T)
# performance_imp
```

### Cross-validation results

```{r}
ranking_imp <- ranking(model_imp_nk)
```

The predictors are ranked in the following order: `r ranking_imp[["fulldata"]]`

```{r, fig.cap= "The cumulative agreement of the ranking in each cross-validation fold"}

cumul_ranking_imp <- plot(cv_proportions(ranking_imp, cumulate = T))+
  xaxis_text(size= 20)+
  yaxis_text(size = 20)
cumul_ranking_imp
```

There is a lot of variability in predictor ranking between folds.

### Select model

```{r}
sugg.size <- suggest_size(model_imp_nk, stat = "elpd")

size <- 9


predictors_final_imp <- head(ranking_imp[["fulldata"]], size) 
```

The suggested size for the model is `r sugg.size` but we will use with the predictors in the following order `r predictors_final_imp`

### Prediction performance

#### Training data

Sensitivity and specificity of our selected model

```{r}
# use the submodel function
sub_train<- submodels_train(model_imp_hs, predictor_ranking = predictors_final_imp, new_data = training_set_imp)

# measure sensitivity, specificity and AUC using cindy's formula for all models
sub_train

colnames <- c(paste0("model",1:length(predictors_final_imp)), "ref")

submodels.sens<- mult.sens(training_set_imp$stbbi.pos, sub_train) # 2000 iter default
submodels.sens

submodels.spec<- mult.spec(training_set_imp$stbbi.pos, sub_train) # 2000 iter default
submodels.spec

submodels.auc<- mult.auc(training_set_imp$stbbi.pos, sub_train)
submodels.auc
```

The specificity is much lower than the sensitivity. We will also add the reference model values.

Combine all values and save to plot later

```{r}
submodels_perf <- bind_rows(submodels.sens, submodels.spec, submodels.auc) %>% 
  rename(model = rowname) %>%
  mutate(performance = rep(c("sensitivity", "specificity", "auc"), each=length(colnames)),
         data = "training")
```

#### Testing data

We fit our submodels with testing data and extract the sensitivity, specificity and AUC

```{r}
sub_test<- submodels_test(sub_train, new_data = testing_set_imp)

# measure sensitivity, specificity and AUC using cindy's formula for all models
sub_test

submodels.sens.t<- mult.sens(testing_set_imp$stbbi.pos, sub_test) # 2000 iter default
submodels.sens.t

submodels.spec.t<- mult.spec(testing_set_imp$stbbi.pos, sub_test) # 2000 iter default
submodels.spec.t

submodels.auc.t<- mult.auc(testing_set_imp$stbbi.pos, sub_test) 
submodels.auc.t
```

Specificity remains very similar regardless of model size

```{r}
# combine
combined <- bind_rows(submodels.sens.t, submodels.spec.t, submodels.auc.t) %>%
  rename(model = rowname) %>%
  mutate(performance = rep(c("sensitivity", "specificity", "auc"), each=length(colnames)),
         data = "testing")


submodels_perf <- submodels_perf %>% bind_rows(combined)
```

## Best performing model

select and save the best performing model

```{r}
n_final<- best(submodels.auc.t)

```

Model `r n_final` is now the sparsest model that performs better than the reference model

```{r}
# select model 2 and transform into array 
array <- as.array(sub_test[[n_final]])

parameters<- dimnames(array)$variable[grep("^b_", dimnames(array)$variable)] # extract the names of parameters to plot 

parameters
# clean parameters
param_clean <- parameters %>% 
  str_replace_all("(b_|\\d+-)| CAD", "") %>%
  str_replace_all("(?<!\\b(?:HCV|HIV|STD|CAD))(?<=\\w)(?=[A-Z])|(?<=\\D)(?=\\d)", " ") %>% 
  str_replace_all(" 2; 000", "2000") %>% 
  str_replace_all("H I V", "HIV") %>% 
  str_replace_all("H C V", "HCV") %>% 
  str_replace_all("S T D", "STD")

param_clean <- c("Intercept", "Past drug injection - Yes", "Type of past STI test - Unspecified", "Type of past STI test - Specified")

color_scheme_set("brightblue")

OR.plot <-
  mcmc_intervals(array %>% exp(), # turn logodds into odds
                 pars = parameters, # select which parameters to show 
                 prob = 0.89, # set 89% CrI
                 prob_outer = 0.95, # set 95% CrI
                 outer_size = 0.5, # set sizes 
                 inner_size = 1, 
                 point_size = 2)+ 
scale_x_continuous(breaks = seq(0,12,1)) +
scale_y_discrete(labels = param_clean)+ # set limits
  geom_vline(xintercept = 1, color = "#36325c", alpha = 0.5)+  
  # geom_vline(xintercept = 0, color = "white")+
  labs(x = "Odds Ratio")

OR.plot
```

Extract the OR of all predictors into a dataframe

```{r}
summary(sub_test[[n_final]], prob = 0.89)

OR_final <- exp(fixef(sub_test[[n_final]], prob = c(0.055, 0.945, 0.025, 0.975))) %>% round(2) %>% as.data.frame()

OR_final

```

### Save files

Only save if there is no overfitting issue

```{r}
#save
if(sum(submodels.auc.t$Q50) <= sum(submodels.auc$Q50)){
write.csv(submodels_perf, "3_intermediate/projpred/submodels_performance.csv", row.names = F)
ggsave("4_outputs/OR_final_model.png", OR.plot, device = png, width = 7, height = 5)
write.csv(OR_final, file = "4_outputs/OR_final.csv")
ggsave("4_outputs/posteriors_full_model_rev.png",mcmc_imp, device= "png", width = 16, height = 14)
ggsave("3_intermediate/projpred/ppc_density_outcome.png", 
       ppc_imp, device = "png", width = 16, height = 7)
}else{
  print("Error: overfitting")
}
```

## **Reference:**

Piironen J, Paasiniemi M, Catalina A, Weber F, Vehtari A (2023). "projpred: Projection Predictive Feature Selection." R package version 2.8.0,Â <https://mc-stan.org/projpred/>.

See vignettes for more information
